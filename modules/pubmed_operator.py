import requests
import xml.etree.ElementTree as ET
import re
from datetime import datetime, timedelta
from typing import List, Optional

def calculate_date_range(mindate=None, maxdate=None, days=None): 
    """ æ¤œç´¢æœŸé–“ã®è¨ˆç®—
    
    - mindateã¨maxdateã‚’æŒ‡å®šâ†’ãã®æœŸé–“ã‚’ä½¿ç”¨
    - daysã®ã¿æŒ‡å®š:maxdateã‚’ä»Šæ—¥ã¨ã—ã¦ã€ãã“ã‹ã‚‰daysåˆ†é¡ã‚‹
    - mindateã¨daysã‚’æŒ‡å®š:maxdateã‚’mindate + daysã¨ã—ã¦æ±ºå®š
    - ã„ãšã‚Œã‚‚æŒ‡å®š:ä»Šæ—¥ã‹ã‚‰éå»1é€±é–“ã¨ã™ã‚‹

    Args:
        mindate: 'YYYY/MM/DD' å½¢å¼ã®æ–‡å­—åˆ— ã¾ãŸã¯ None 
        maxdate: 'YYYY/MM/DD' å½¢å¼ã®æ–‡å­—åˆ— ã¾ãŸã¯ None 
        days: int æ—¥æ•°, maxdateã‹ã‚‰é¡ã‚‹æœŸé–“
    Returns: 
        (mindate_str, maxdate_str) ã®æ–‡å­—åˆ—ã‚¿ãƒ—ãƒ«
    """
    # Get today's date
    today = datetime.today()

    # Determine maxdate
    if maxdate:
        max_date_obj = datetime.strptime(maxdate, '%Y/%m/%d')
    else:
        max_date_obj = today

    # Determine mindate
    if mindate:
        mindate_obj = datetime.strptime(mindate, '%Y/%m/%d')
    elif days:
        mindate_obj = max_date_obj - timedelta(days=days)
    else:
        mindate_obj = max_date_obj - timedelta(days=7) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1é€±é–“

    # Convert back to string format
    mindate_str = mindate_obj.strftime('%Y/%m/%d')
    maxdate_str = max_date_obj.strftime('%Y/%m/%d')

    return mindate_str, maxdate_str

def fetch_esearch(keywords:list[str], min_date, max_date, retmax=100):
    '''PubMedã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æ—¥ä»˜ã«åŸºã¥ã„ã¦è«–æ–‡IDã‚’æ¤œç´¢ã™ã‚‹é–¢æ•°
    Args:
        keywords (list[str]): æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        min_date (str): æ¤œç´¢æ—¥ä»˜ (YYYY/MM/DD)
        Max_date (str): æ¤œç´¢æ—¥ä»˜ (YYYY/MM/DD)
        retmax (int, optional): å–å¾—ã™ã‚‹æœ€å¤§è«–æ–‡æ•°. Defaults to 100.
    Returns:
        list[str]: æ¤œç´¢ã§å–å¾—ã—ãŸè«–æ–‡IDã®ãƒªã‚¹ãƒˆ
    '''
    search_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'
    query = ' AND '.join(keywords)

    search_params = {
        'db': 'pubmed',
        'term': query,
        'mindate': min_date,
        'maxdate': max_date,
        'retmode': 'xml',
        'retmax': retmax
    }
    response = requests.get(search_url, params=search_params)
    pmids = re.findall(r'<Id>(\d+)</Id>', response.text)
    return pmids

def fetch_esummary(pmids:list[str]) -> str:
    '''PubMedã§è«–æ–‡IDã«åŸºã¥ã„ã¦è«–æ–‡æƒ…å ±ã‚’å–å¾—ã™ã‚‹é–¢æ•°
    Args:
        ids (list[str]): è«–æ–‡IDã®ãƒªã‚¹ãƒˆ
    Returns:
        str: è«–æ–‡æƒ…å ±ã‚’å«ã‚€XMLæ–‡å­—åˆ—
    '''
    if not pmids:
        return 'No results found.'

    summary_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi'
    summary_params = {
        'db': 'pubmed',
        'id': ','.join(pmids),
        'retmode': 'xml'
    }

    response = requests.get(summary_url, params=summary_params)
    return response.text

def parse_esummary_xml(xml_data:str) -> list[dict]:
    """ PubMedã®XMLãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°

    Args:
        xml_data (_type_): fetch_abstracté–¢æ•°ã§å–å¾—ã—ãŸXMLãƒ‡ãƒ¼ã‚¿

    Returns:
        list[dict]: è«–æ–‡æƒ…å ±ã®ãƒªã‚¹ãƒˆï¼ˆå„è«–æ–‡ã¯è¾æ›¸å½¢å¼ã§ã‚¿ã‚¤ãƒˆãƒ«ã€DOIã€ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’å«ã‚€ï¼‰
    """
    root = ET.fromstring(xml_data)
    esummary = []
    for docsum in root.findall('.//DocSum'):
        title = docsum.find('Item[@Name="Title"]').text
        pubdate = docsum.find('Item[@Name="PubDate"]').text
        pmid = docsum.find('Item[@Name="ArticleIds"]/Item[@Name="pubmed"]').text

        doi_raw = docsum.find('Item[@Name="ELocationID"]').text if docsum.find('Item[@Name="ELocationID"]') is not None else 'N/A'
        doi = extract_doi(doi_raw)
        url = doi_to_url(doi)

        esummary.append({'pmid':pmid, 'Title': title, 'pubdate':pubdate, 'URL': url})
    return esummary

def extract_doi(text: str) -> str:
    """
    æ··åœ¨ã—ãŸ ELocationID æ–‡å­—åˆ—ã‹ã‚‰ DOI ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°ã€‚
    DOI ãŒãªã‘ã‚Œã° 'N/A' ã‚’è¿”ã™ã€‚
    """
    if not text:
        return "N/A"
    
    # DOI ã®ä¸€èˆ¬çš„ãªæ­£è¦è¡¨ç¾ï¼ˆ10. ã‹ã‚‰å§‹ã¾ã‚‹ï¼‰
    doi_pattern = r"(10\.\d{4,9}/[^\s]+)"
    match = re.search(doi_pattern, text)

    return match.group(1) if match else "N/A"

def doi_to_url(doi: str) -> str:
    """
    DOI ã‹ã‚‰ãƒªãƒ³ã‚¯ URL ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    DOI ãŒç„¡åŠ¹ã¾ãŸã¯ 'N/A' ã®å ´åˆã¯ç©ºæ–‡å­—ã‚’è¿”ã™ã€‚
    """
    if not doi or doi == "N/A":
        return ""
    return f"https://doi.org/{doi}"


def fetch_eFetch(pmids: List[str]) -> dict[str, str]:
    """ PubMedã‹ã‚‰è«–æ–‡ã®ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’å–å¾—ã™ã‚‹é–¢æ•°
    Args:
        pmids (list[str]): è«–æ–‡IDã®ãƒªã‚¹ãƒˆ
    Returns:
        dict[str, str]: {pmid: abstract_text}
    """
    if not pmids:
        return {}

    efetch_url = (
        "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?"
        "db=pubmed&retmode=xml&id=" + ",".join(pmids)
    )

    response = requests.get(efetch_url)
    response.raise_for_status()

    root = ET.fromstring(response.content)

    abstracts = {}

    for article in root.findall(".//PubmedArticle"):
        pmid_elem = article.find(".//PMID")
        pmid = pmid_elem.text if pmid_elem is not None else None

        abstract_elem = article.find(".//Abstract/AbstractText")
        abstract_text = (
            abstract_elem.text if abstract_elem is not None else "N/A"
        )

        if pmid:
            abstracts[pmid] = abstract_text

    return abstracts

def fetch_weekly_counts(keywords: list[str], weeks: int = 12):
    """
    éå»Né€±é–“åˆ†ã‚’1é€±é–“å˜ä½ã§åŒºåˆ‡ã£ã¦ PubMed ã®ãƒ’ãƒƒãƒˆä»¶æ•°ã‚’è¿”ã™ã€‚
    
    Args:
        keywords (list[str]): æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        weeks (int): é¡ã‚‹é€±æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ12é€±é–“ï¼‰
    
    Returns:
        dict: { "YYYY/MM/DD": int } å„é€±ã®é–‹å§‹æ—¥ã®ãƒ’ãƒƒãƒˆä»¶æ•°
    """
    today = datetime.today()
    start_date = today - timedelta(weeks=weeks)

    results = {}
    cursor = start_date

    while cursor < today:
        week_start = cursor
        week_end = cursor + timedelta(days=6)

        # æœªæ¥ã‚’è¶…ãˆãªã„ã‚ˆã†èª¿æ•´
        if week_end > today:
            week_end = today

        # æ–‡å­—åˆ—ã¸
        min_date = week_start.strftime('%Y/%m/%d')
        max_date = week_end.strftime('%Y/%m/%d')

        # PubMed ESearch API
        search_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'
        query = ' AND '.join(keywords)

        search_params = {
            'db': 'pubmed',
            'term': query,
            'mindate': min_date,
            'maxdate': max_date,
            'retmode': 'xml',
            'retmax': 1  # ä»¶æ•°ç¢ºèªã ã‘ãªã®ã§1ä»¶ã§ååˆ†
        }

        response = requests.get(search_url, params=search_params)

        # `<Count>123</Count>` ã‚’æŠ½å‡º
        m = re.search(r'<Count>(\d+)</Count>', response.text)
        count = int(m.group(1)) if m else 0

        # é€±ã®é–‹å§‹æ—¥ã‚’ã‚­ãƒ¼ã«ä¿å­˜
        results[min_date] = count

        # æ¬¡ã®é€±ã¸
        cursor += timedelta(days=7)

    return results

# ãƒ¡ã‚¤ãƒ³å‡¦ç† (ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢)
if __name__ == '__main__':
    
    keywords_input = input('æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›ã—ã¦ãã ã•ã„: ')
    keywords = [kw.strip() for kw in keywords_input.split(',')]

    # -------------------------
    # ğŸ”¸ éå»Né€±é–“ã®ãƒ’ãƒƒãƒˆæ•°ã‚’ç¢ºèª
    # -------------------------
    try:
        weeks = int(input("éå»ä½•é€±é–“ã®è«–æ–‡æ•°ã‚’è¡¨ç¤ºã—ã¾ã™ã‹ï¼Ÿï¼ˆä¾‹ï¼š12ï¼‰: "))
    except ValueError:
        weeks = 12  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        print("æ•°å€¤ä»¥å¤–ãŒå…¥åŠ›ã•ã‚ŒãŸãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ12é€±é–“ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

    weekly_counts = fetch_weekly_counts(keywords, weeks)

    print("\n=== éå»ã®è«–æ–‡ãƒ’ãƒƒãƒˆæ•°ï¼ˆ1é€±é–“å˜ä½ï¼‰ ===")
    for week_start, count in weekly_counts.items():
        print(f"{week_start} ã€œ : {count} ä»¶")

    print("\n----------------------------------------\n")

    # -------------------------
    # ğŸ”¸ ä»Šé€± (ç›´è¿‘1é€±é–“) ã®è«–æ–‡ä¸€è¦§ã‚’å–å¾—
    # -------------------------
    mindate, maxdate = calculate_date_range()

    print(f'æ¤œç´¢æœŸé–“: {mindate} ï½ {maxdate}')
    print(f'æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}')

    pmids = fetch_esearch(keywords, mindate, maxdate)

    if not pmids:
        print('è©²å½“ã™ã‚‹è«–æ–‡ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')
        exit()

    print(f'{len(pmids)} ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸã€‚')

    # è«–æ–‡æƒ…å ±ã‚’å–å¾—
    esummary_xml = fetch_esummary(pmids)
    esummary_dict = parse_esummary_xml(esummary_xml)

    # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’å–å¾—
    abstracts_dict = fetch_eFetch(pmids)

    # -------------------------
    # ğŸ”¸ è«–æ–‡æƒ…å ±ã‚’è¡¨ç¤º
    # -------------------------
    for i, paper in enumerate(esummary_dict, start=1):
        pmid = paper['pmid']
        abstract = abstracts_dict.get(pmid, "N/A")

        print(f"\n=== è«–æ–‡ {i} ===")
        print(f"PMID: {pmid}")
        print(f"ã‚¿ã‚¤ãƒˆãƒ«: {paper['Title']}")
        print(f"å‡ºç‰ˆæ—¥: {paper['pubdate']}")
        print(f"URL: {paper['URL']}")
        print(f"ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ: {abstract}")